{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c545ca2-ac18-43b3-b95a-0acfa8dc4799",
   "metadata": {},
   "source": [
    "# Diferenças entre AdaBoost e Gradient Boosting (GBM)\n",
    "\n",
    "| **Aspecto**                   | **AdaBoost**                                                                 | **Gradient Boosting (GBM)**                                                     |\n",
    "|--------------------------------|------------------------------------------------------------------------------|--------------------------------------------------------------------------------|\n",
    "| **Tamanho das Árvores**        | Usa árvores pequenas, geralmente de apenas uma folha (árvores de decisão \"stumps\"). | Permite o uso de árvores de tamanhos variáveis, conforme configurado.           |\n",
    "| **Método de Atualização**      | Não realiza uma verificação explícita da resposta; ajusta os pesos diretamente. | Usa uma soma dos erros para ajustar o modelo, otimizando uma função de perda.   |\n",
    "| **Estratégia de Combinação**   | Combina os modelos base ponderando os erros por reponderação dos pesos.      | Combina os modelos base utilizando gradiente descendente em uma função de perda. |\n",
    "| **Robustez a Overfitting**     | Mais sensível a overfitting em dados ruidosos.                              | Pode ser regularizado para reduzir overfitting por meio de parâmetros como learning rate. |\n",
    "| **Velocidade de Treinamento**  | Geralmente mais rápido em conjuntos pequenos de dados.                      | Pode ser mais lento devido à complexidade de otimizar a função de perda.        |\n",
    "| **Flexibilidade de Funções de Perda** | Suporta apenas funções de perda específicas (e.g., classificação ou regressão simples). | Oferece maior flexibilidade, permitindo diferentes funções de perda.           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e335d5be-fa9c-41c5-a045-8cb037aa3a64",
   "metadata": {},
   "source": [
    "# exemplo de aplicação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "effd4244-6aea-47cf-bb29-691c8f2a5f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc18d50c-c7a2-43c1-b1f4-6c23d3184066",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09182320-d3bb-4c2f-a526-09a41cdd1e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e2993f-1a4b-4a90-aad1-2f1bd617624e",
   "metadata": {},
   "source": [
    "# Hiperparâmetros mais importantes do Gradient Boosting (GBM)\n",
    "\n",
    "1. **`n_estimators`**  \n",
    "   Número de árvores (ou modelos fracos) a serem treinados. Um valor maior pode aumentar o desempenho, mas também o risco de overfitting.  \n",
    "   - **Default:** 100  \n",
    "\n",
    "2. **`learning_rate`**  \n",
    "   Taxa de aprendizado que controla a contribuição de cada árvore para o modelo final. Valores menores requerem mais árvores (`n_estimators`).  \n",
    "   - **Default:** 0.1  \n",
    "\n",
    "3. **`max_depth`**  \n",
    "   Profundidade máxima das árvores. Limita o crescimento para evitar overfitting.  \n",
    "   - **Default:** 3  \n",
    "\n",
    "4. **`min_samples_split`**  \n",
    "   Número mínimo de amostras necessárias para dividir um nó. Controla o nível de segmentação da árvore.  \n",
    "   - **Default:** 2  \n",
    "\n",
    "5. **`subsample`**  \n",
    "   Proporção de amostras a serem usadas para treinar cada árvore. Usar menos de 1.0 introduz aleatoriedade, ajudando a reduzir o overfitting.  \n",
    "   - **Default:** 1.0  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc390bf-aeb1-4350-bbee-fcae7349676a",
   "metadata": {},
   "source": [
    "# Diferença entre GBM e Stochastic GBM\n",
    "\n",
    "A principal diferença entre o **Gradient Boosting Machine (GBM)** tradicional e o **Stochastic Gradient Boosting Machine (Stochastic GBM)**, descrito no artigo de Jerome Friedman, é o uso de amostragem aleatória de dados em cada iteração do algoritmo Stochastic GBM, enquanto o GBM tradicional utiliza todo o conjunto de dados.\n",
    "\n",
    "## Diferença-principal:\n",
    "- **GBM (Tradicional)**:\n",
    "  - Usa todo o conjunto de dados para treinar cada árvore em cada etapa.\n",
    "  - É mais determinístico, mas pode ser suscetível ao overfitting, especialmente em dados ruidosos.\n",
    "\n",
    "- **Stochastic GBM**:\n",
    "  - Em cada iteração, utiliza apenas uma amostra aleatória dos dados (definida pelo parâmetro `subsample`).\n",
    "  - Introduz variabilidade no processo de treinamento, melhorando a **generalização** e reduzindo o overfitting.\n",
    "\n",
    "## Vantagens do Stochastic GBM:\n",
    "1. **Melhor Generalização**: A aleatoriedade reduz a tendência de overfitting.\n",
    "2. **Desempenho Mais Rápido**: Processa um subconjunto menor de dados por iteração.\n",
    "3. **Robustez ao Ruído**: É menos influenciado por outliers ou padrões ruidosos nos dados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7351ab-b95f-4e73-8d77-6f8ed13d604b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
